{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig\n",
    "from datasets import Dataset\n",
    "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear unused memory in GPU\n",
    "torch.cuda.empty_cache()\n",
    "torch.cuda.reset_peak_memory_stats()\n",
    "# avoid fragmentation (out of memory error)\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step 1: load the opt model and tokenizer\n",
    "model_name = \"facebook/opt-2.7b\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)\n",
    "\n",
    "# Prepare model for low-rank adaptation\n",
    "model = prepare_model_for_kbit_training(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Load and format your dataset\n",
    "def load_custom_dataset(json_file):\n",
    "    with open(json_file, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    headlines = [item['headline'] for item in data]\n",
    "    labels = [item['label'] for item in data]\n",
    "    return Dataset.from_dict({'text': headlines, 'label': labels})\n",
    "\n",
    "dataset = load_custom_dataset('tbd.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Tokenize the dataset\n",
    "# include labels for loss\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./opt-finetuned\",       # Directory to save the fine-tuned model          \n",
    "    eval_strategy='no',       # Disable evaluation, we don't have evaluation dataset\n",
    "    learning_rate=5e-5,                  # Learning rate\n",
    "    per_device_train_batch_size=6,       # Batch size per GPU\n",
    "    #gradient_accumulation_steps=4,  # Simulate a batch size of 2 * 4 = 8\n",
    "    num_train_epochs=50,                  # Number of training epochs\n",
    "    logging_dir=\"./logs\",                # Directory for logs\n",
    "    logging_steps=100,                    # Log frequency\n",
    "    save_strategy=\"epoch\",               # Save checkpoint after each epoch\n",
    "    fp16=True,                           # Enable mixed precision training\n",
    "    #gradient_checkpointing=True,\n",
    "    #gradient_checkpointing_kwargs ={\"use_reentrant\": False}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Initialize the Trainer\n",
    "# Trainer() automatically uses GPU if necessary \n",
    "# libraries are installed and GPU is available\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Fine-tune the model\n",
    "print('Training model...')\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 7: Save the fine-tuned model\\\n",
    "print('Saving model...')\n",
    "trainer.save_model(\"./opt-finetuned\")\n",
    "tokenizer.save_pretrained(\"./opt-finetuned\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
